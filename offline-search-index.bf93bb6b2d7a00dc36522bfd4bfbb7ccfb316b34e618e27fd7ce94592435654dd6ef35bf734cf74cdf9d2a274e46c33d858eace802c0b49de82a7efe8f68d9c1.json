[{"body":"Thanks for taking the time to join our community and start contributing!\nThe Contributor Guide provides detailed instructions on how to get your ideas and bug fixes seen and accepted.\nPlease remember to sign the CNCF CLA and read and observe the Code of Conduct.\n","excerpt":"Thanks for taking the time to join our community and start ‚Ä¶","ref":"/azuredisk-csi-driver/contribute/contributing/","title":"Contributing"},{"body":"CSI driver E2E usage example 1. create a pod with csi azuredisk driver mount on linux Option#1: Azuredisk Dynamic Provisioning  Create an azuredisk CSI storage class  kubectl create -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/storageclass-azuredisk-csi.yaml  Create an azuredisk CSI PVC  kubectl create -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/pvc-azuredisk-csi.yaml Option#2: Azuredisk Static Provisioning(use an existing azure disk)  Create an azuredisk CSI PV, download pv-azuredisk-csi.yaml file and edit diskName, diskURI in volumeAttributes  wget https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/pv-azuredisk-csi.yaml vi pv-azuredisk-csi.yaml kubectl create -f pv-azuredisk-csi.yaml  Create an azuredisk CSI PVC which would be bound to the above PV  kubectl create -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/pvc-azuredisk-csi-static.yaml 2. validate PVC status and create an nginx pod  make sure pvc is created and in Bound status finally  watch kubectl describe pvc pvc-azuredisk  create a pod with azuredisk CSI PVC  kubectl create -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/nginx-pod-azuredisk.yaml 3. enter the pod container to do validation  watch the status of pod until its Status changed from Pending to Running and then enter the pod container  $ watch kubectl describe po nginx-azuredisk $ kubectl exec -it nginx-azuredisk -- bash Filesystem Size Used Avail Use% Mounted on overlay 30G 15G 15G 52% / ... /devhost/sdc 9.8G 37M 9.8G 1% /mnt/azuredisk ... In the above example, there is a /mnt/azuredisk directory mounted as disk filesystem.\n","excerpt":"CSI driver E2E usage example 1. create a pod with csi azuredisk driver ‚Ä¶","ref":"/azuredisk-csi-driver/example/e2e/","title":"E2E usage example"},{"body":"Raw Block Volume Example  Specify volumeMode as Block in PVC   volumeMode is Filesystem by default\n apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvc-azurediskspec:accessModes:-ReadWriteOnceresources:requests:storage:10GivolumeMode:BlockstorageClassName:managed-csiSpecify volumeDevices, devicePath in Pod Spec  kind:PodapiVersion:v1metadata:name:nginx-azurediskspec:nodeSelector:kubernetes.io/os:linuxcontainers:-image:nginxname:nginx-azurediskvolumeDevices:-name:azurediskdevicePath:/dev/sdxvolumes:-name:azurediskpersistentVolumeClaim:claimName:pvc-azurediskCheck block device in pod  # kubectl exec -it nginx-azuredisk bash root@nginx-azuredisk:/# dd if=/dev/zero of=/dev/sdx bs=1024k count=100 100+0 records in 100+0 records out 104857600 bytes (105 MB, 100 MiB) copied, 0.0566293 s, 1.9 GB/s ","excerpt":"Raw Block Volume Example  Specify volumeMode as Block in PVC ‚Ä¶","ref":"/azuredisk-csi-driver/features/rawblockvolume/","title":"Raw Block Volume feature"},{"body":"","excerpt":"","ref":"/azuredisk-csi-driver/blog/releases/","title":"Release Notes"},{"body":"Prerequisite  The driver depends on cloud provider config file, usually it's /etc/kubernetes/azure.json on all kubernetes nodes deployed by AKS or aks-engine, here is azure.json example.   To specify a different cloud provider config file, create azure-cred-file configmap before driver installation, e.g. for OpenShift, it's /etc/kubernetes/cloud.conf (make sure config file path is in the volumeMounts.mountPath)\nkubectl create configmap azure-cred-file --from-literal=path=\"/etc/kubernetes/cloud.conf\" --from-literal=path-windows=\"C:\\\\k\\\\cloud.conf\" -n kube-system   This driver also supports read cloud config from kuberenetes secret. If cluster identity is Managed Service Identity(MSI), make sure user assigned identity has Contributor role on node resource group  ","excerpt":"Prerequisite  The driver depends on cloud provider config file, ‚Ä¶","ref":"/azuredisk-csi-driver/development/dependencies/","title":"Dependency Management"},{"body":"","excerpt":"","ref":"/azuredisk-csi-driver/faq/known-issues/","title":"Known Issues"},{"body":"Security Announcements Join the kubernetes-security-announce group for security and vulnerability announcements.\nYou can also subscribe to an RSS feed of the above using this link.\nReporting a Vulnerability Instructions for reporting a vulnerability can be found on the Kubernetes Security and Disclosure Information page.\nSupported Versions Information about supported Kubernetes versions can be found on the Kubernetes version and version skew support policy page on the Kubernetes website.\n","excerpt":"Security Announcements Join the kubernetes-security-announce group for ‚Ä¶","ref":"/azuredisk-csi-driver/contribute/security/","title":"Security Policy"},{"body":"Shared disk(Multi-node ReadWrite) Feature Status: Alpha Azure shared disks (preview) is a new feature for Azure managed disks that enables attaching an Azure managed disk to multiple virtual machines (VMs) simultaneously. Attaching a managed disk to multiple VMs allows you to either deploy new or migrate existing clustered applications to Azure.\nNote Only raw block device(volumeMode: Block) is supported on shared disk feature, kubernetes application should manage coordination and control of writes, reads, locks, caches, mounts, fencing on the shared disk which is exposed as raw block device.\nExample  Create Storage Class and PVC  apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:managed-csiprovisioner:disk.csi.azure.comparameters:skuname:Premium_LRS# Currently shared disk only available with premium SSDmaxShares:\"2\"cachingMode:None# ReadOnly cache is not available for premium SSD with maxShares\u003e1reclaimPolicy:Delete---kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvc-azurediskspec:accessModes:-ReadWriteManyresources:requests:storage:256Gi# minimum size of shared disk is 256GB (P15)volumeMode:BlockstorageClassName:managed-csiCreate a deployment with 2 replicas and specify volumeDevices, devicePath in Spec  apiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginxname:deployment-azurediskspec:replicas:2selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxname:deployment-azurediskspec:containers:-name:deployment-azurediskimage:nginxvolumeDevices:-name:azurediskdevicePath:/dev/sdxvolumes:-name:azurediskpersistentVolumeClaim:claimName:pvc-azurediskCheck block device in pod  # kubectl exec -it deployment-sharedisk-7454978bc6-xh7jp bash root@deployment-sharedisk-7454978bc6-xh7jp:/# dd if=/dev/zero of=/dev/sdx bs=1024k count=100 100+0 records in 100+0 records out 104857600 bytes (105 MB, 100 MiB) copied, 0.0502999 s, 2.1 GB/s ","excerpt":"Shared disk(Multi-node ReadWrite) Feature Status: Alpha Azure shared ‚Ä¶","ref":"/azuredisk-csi-driver/features/shareddisk/","title":"Shared disk feature"},{"body":"Install CSI driver master version\nInstall v0.8.0 CSI driver\nInstall v0.7.0 CSI driver\nInstall v0.6.0 CSI driver\n","excerpt":"Install CSI driver master version\nInstall v0.8.0 CSI driver\nInstall ‚Ä¶","ref":"/azuredisk-csi-driver/install/azuredisk/","title":"Install AzureDisk CSI Driver"},{"body":"To be completed.\n","excerpt":"To be completed.\n","ref":"/azuredisk-csi-driver/development/design-docs/","title":"Design Docs and KEPs"},{"body":"","excerpt":"","ref":"/azuredisk-csi-driver/development/e2e/","title":"E2E tests"},{"body":"Azure Disk Snapshot feature  Snapshot feature is beta since Kubernetes v1.17.0, refer to Snapshot \u0026 Restore Feature for more details.  Introduction This driver supports both full and incremental snapshot functionalities, user could set incremental in VolumeSnapshotClass to control whether create full or incremental(by default) snapshot(refer to VolumeSnapshotClass for detailed parameters description):\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotClass metadata: name: csi-azuredisk-vsc driver: disk.csi.azure.com deletionPolicy: Delete parameters: incremental: \"true\" # available values: \"true\"(by default), \"false\" Install CSI Driver Follow the instructions to install snapshot driver.\n1. Create source PVC and an example pod to write data kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/storageclass-azuredisk-csi.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/pvc-azuredisk-csi.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/nginx-pod-azuredisk.yaml  Check source PVC  $ kubectl exec nginx-azuredisk -- ls /mnt/azuredisk lost+found outfile 2. Create a snapshot on source PVC kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/snapshot/storageclass-azuredisk-snapshot.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/snapshot/azuredisk-volume-snapshot.yaml  Check snapshot Status  $ kubectl describe volumesnapshot azuredisk-volume-snapshot Name: azuredisk-volume-snapshot Namespace: default Labels: \u003cnone\u003e Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"snapshot.storage.k8s.io/v1beta1\",\"kind\":\"VolumeSnapshot\",\"metadata\":{\"annotations\":{},\"name\":\"azuredisk-volume-snapshot\",\"n... API Version: snapshot.storage.k8s.io/v1beta1 Kind: VolumeSnapshot Metadata: Creation Timestamp: 2020-02-04T13:59:54Z Finalizers: snapshot.storage.kubernetes.io/volumesnapshot-as-source-protection snapshot.storage.kubernetes.io/volumesnapshot-bound-protection Generation: 1 Resource Version: 48044 Self Link: /apis/snapshot.storage.k8s.io/v1beta1/namespaces/default/volumesnapshots/azuredisk-volume-snapshot UID: 2b0ef334-4112-4c86-8360-079c625d5562 Spec: Source: Persistent Volume Claim Name: pvc-azuredisk Volume Snapshot Class Name: csi-azuredisk-vsc Status: Bound Volume Snapshot Content Name: snapcontent-2b0ef334-4112-4c86-8360-079c625d5562 Creation Time: 2020-02-04T14:23:36Z Ready To Use: true Restore Size: 10Gi Events: \u003cnone\u003e  In above example, snapcontent-2b0ef334-4112-4c86-8360-079c625d5562 is the snapshot name\n 3. Create a new PVC based on snapshot kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/snapshot/pvc-azuredisk-snapshot-restored.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/snapshot/nginx-pod-restored-snapshot.yaml  Check data  $ kubectl exec nginx-restored -- ls /mnt/azuredisk lost+found outfile Links  CSI Snapshotter  ","excerpt":"Azure Disk Snapshot feature  Snapshot feature is beta since Kubernetes ‚Ä¶","ref":"/azuredisk-csi-driver/features/snapshot/","title":"Snapshot feature"},{"body":"CSI driver on Windows Feature Status: Alpha CSI on Windows support is an alpha feature since Kubernetes v1.18, refer to Windows-CSI-Support for more details.\nPrerequisite  Install CSI-Proxy on Windows Node  csi-proxy installation is supported with aks-engine v0.48.0 or higher version\nDeploy a Windows pod with PVC mount Create Storage Class kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/storageclass-azuredisk-csi.yaml Create Windows pod kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/windows/statefulset.yaml enter pod container to do validation # kubectl exec -it busybox-azuredisk-0 cmd Microsoft Windows [Version 10.0.17763.1098] (c) 2018 Microsoft Corporation. All rights reserved. C:\\\u003ecd c:\\mnt\\azuredisk c:\\mnt\\azuredisk\u003edir Volume in drive C has no label. Volume Serial Number is C820-6BEE Directory of c:\\mnt\\azuredisk 05/31/2020 12:41 PM 528 data.txt 1 File(s) 528 bytes 0 Dir(s) 107,268,366,336 bytes free c:\\mnt\\azuredisk\u003ecat data.txt 2020-05-31 12:40:59Z 2020-05-31 12:41:00Z 2020-05-31 12:41:01Z 2020-05-31 12:41:02Z In the above example, there\n","excerpt":"CSI driver on Windows Feature Status: Alpha CSI on Windows support is ‚Ä¶","ref":"/azuredisk-csi-driver/features/windows/","title":"Windows feature"},{"body":"Driver Parameters  storage class disk.csi.azure.com parameters are compatible with in-tree kubernetes.io/azure-disk driver\n Dynamic Provisioning  get an example\n    Name Meaning Available Value Mandatory Default value     skuName azure disk storage account type (alias: storageAccountType) Standard_LRS, Premium_LRS, StandardSSD_LRS, UltraSSD_LRS No StandardSSD_LRS   kind managed or unmanaged(blob based) disk managed (dedicated, shared are deprecated) No managed   fsType File System Type ext4, ext3, ext2, xfs No ext4   cachingMode Azure Data Disk Host Cache Setting None, ReadOnly, ReadWrite No ReadOnly   storageAccount specify the storage account name in which azure disk will be created STORAGE_ACCOUNT_NAME No if empty, driver will find a suitable storage account that matches skuName in the same resource group as current k8s cluster   location specify the Azure location in which azure disk will be created eastus, westus, etc. No if empty, driver will use the same location name as current k8s cluster   resourceGroup specify the resource group in which azure disk will be created existing resource group name No if empty, driver will use the same resource group name as current k8s cluster   DiskIOPSReadWrite UltraSSD disk IOPS Capability (minimum: 2 IOPS/GiB ) 100~160000 No 500   DiskMBpsReadWrite UltraSSD disk Throughput Capability(minimum: 0.032/GiB) 1~2000 No 100   tags azure disk tags tag format: key1=val1,key2=val2 No \"‚Äù   diskEncryptionSetID ResourceId of the disk encryption set to use for enabling encryption at rest format: /subscriptions/{subs-id}/resourceGroups/{rg-name}/providers/Microsoft.Compute/diskEncryptionSets/{diskEncryptionSet-name} No \"‚Äù   writeAcceleratorEnabled Write Accelerator on Azure Disks true, false No \"‚Äù    Static Provisioning(bring your own Azure Disk)  get an example\n    Name Meaning Available Value Mandatory Default value     volumeHandle Azure disk URI /subscriptions/{sub-id}/resourcegroups/{group-name}/providers/microsoft.compute/disks/{disk-id} Yes N/A   volumeAttributes.fsType File System Type ext4, ext3, xfs No ext4   volumeAttributes.partition partition num of the existing disk 1, 2, 3 No empty(no partition) - make sure partition format is like -part1   volumeAttributes.cachingMode disk host cache setting None, ReadOnly, ReadWrite No ReadOnly    VolumeSnapshotClass    Name Meaning Available Value Mandatory Default value     resourceGroup resource group for storing snapshot shots EXISTING RESOURCE GROUP No If not specified, snapshot will be stored in the same resource group as source Azure disk   tags azure disk tags tag format: ‚Äòkey1=val1,key2=val2‚Äô No \"‚Äù    ","excerpt":"Driver Parameters  storage class disk.csi.azure.com parameters are ‚Ä¶","ref":"/azuredisk-csi-driver/install/configs/","title":"Configure AzureDisk CSI driver"},{"body":"Azure disk CSI driver development guide How to build this project  Clone repo  $ mkdir -p $GOPATH/src/sigs.k8s.io/ $ git clone https://github.com/kubernetes-sigs/azuredisk-csi-driver $GOPATH/src/sigs.k8s.io/azuredisk-csi-driver  Build CSI driver  $ cd $GOPATH/src/sigs.k8s.io/azuredisk-csi-driver $ make azuredisk  Run verification before sending PR  $ make verify How to test CSI driver in local environment  Install csc tool according to https://github.com/rexray/gocsi/tree/master/csc  $ mkdir -p $GOPATH/src/github.com/rexray $ cd $GOPATH/src/github.com/rexray $ git clone https://github.com/rexray/gocsi.git $ cd gocsi/csc $ make build Start CSI driver locally $ cd $GOPATH/src/sigs.k8s.io/azuredisk-csi-driver $ ./_output/azurediskplugin --endpoint tcp://127.0.0.1:10000 --nodeid CSINode -v=5 \u0026  Before running CSI driver, create ‚Äú/etc/kubernetes/azure.json‚Äù file under testing server(it's better copy azure.json file from a k8s cluster with service principle configured correctly) and set AZURE_CREDENTIAL_FILE as following:\n export set AZURE_CREDENTIAL_FILE=/etc/kubernetes/azure.json 1. Get plugin info $ csc identity plugin-info --endpoint tcp://127.0.0.1:10000 \"disk.csi.azure.com\" \"v0.5.0\" 2. Create an azure disk volume $ csc controller new --endpoint tcp://127.0.0.1:10000 --cap 1,block CSIVolumeName --req-bytes 2147483648 --params skuname=Standard_LRS,kind=managed \"/subscriptions/b9d2281e-dcd5-4dfd-9a97-xxx/resourceGroups/xxx/providers/Microsoft.Compute/disks/pvc-disk-dynamic-398b838f-0432-11e9-9978-000d3a00df41\" 2147483648 \"kind\"=\"managed\" \"skuname\"=\"Standard_LRS\" 3. Attach an Azure disk volume to a node $ csc controller publish --endpoint tcp://127.0.0.1:10000 --node-id k8s-agentpool-17181929-0 --cap 1,block \"/subscriptions/b9d2281e-dcd5-4dfd-9a97-xxx/resourceGroups/xxx/providers/Microsoft.Compute/disks/pvc-disk-dynamic-398b838f-0432-11e9-9978-000d3a00df41\" 4. Stage an Azure disk volume on a node (format and mount disk to a staging path) $ csc node stage --endpoint tcp://127.0.0.1:10000 --cap 1,block --staging-target-path=/tmp/staging-path --pub-info devicePath=\"0\" \"/subscriptions/b9d2281e-dcd5-4dfd-9a97-xxx/resourceGroups/xxx/providers/Microsoft.Compute/disks/pvc-disk-dynamic-398b838f-0432-11e9-9978-000d3a00df41\" 5. Publish an Azure disk volume on a node (bind mount the volume from staging to target path) $ csc node publish --endpoint tcp://127.0.0.1:10000 --cap 1,block --staging-target-path=/tmp/staging-path --target-path=/tmp/publish-path \"/subscriptions/b9d2281e-dcd5-4dfd-9a97-xxx/resourceGroups/xxx/providers/Microsoft.Compute/disks/pvc-disk-dynamic-398b838f-0432-11e9-9978-000d3a00df41\" 6. Unpublish an Azure disk volume on a node $ csc node unpublish --endpoint tcp://127.0.0.1:10000 --target-path=/tmp/publish-path \"/subscriptions/b9d2281e-dcd5-4dfd-9a97-xxx/resourceGroups/xxx/providers/Microsoft.Compute/disks/pvc-disk-dynamic-398b838f-0432-11e9-9978-000d3a00df41\" 7. Unstage an Azure disk volume on a node $ csc node unstage --endpoint tcp://127.0.0.1:10000 --staging-target-path=/tmp/staging-path \"/subscriptions/b9d2281e-dcd5-4dfd-9a97-xxx/resourceGroups/xxx/providers/Microsoft.Compute/disks/pvc-disk-dynamic-398b838f-0432-11e9-9978-000d3a00df41\" 8. Detach an Azure disk volume from a node $ csc controller unpublish --endpoint tcp://127.0.0.1:10000 --node-id k8s-agentpool-17181929-0 \"/subscriptions/b9d2281e-dcd5-4dfd-9a97-xxx/resourceGroups/xxx/providers/Microsoft.Compute/disks/pvc-disk-dynamic-398b838f-0432-11e9-9978-000d3a00df41\" 9. Delete an Azure disk volume $ csc controller del --endpoint tcp://127.0.0.1:10000 \"/subscriptions/b9d2281e-dcd5-4dfd-9a97-xxx/resourceGroups/xxx/providers/Microsoft.Compute/disks/pvc-disk-dynamic-398b838f-0432-11e9-9978-000d3a00df41\" 10. Validate volume capabilities $ csc controller validate-volume-capabilities --endpoint tcp://127.0.0.1:10000 --cap 1,block CSIVolumeID CSIVolumeID true 11. Get NodeID $ csc node get-info --endpoint tcp://127.0.0.1:10000 CSINode 12. Create snapshot $ csc controller create-snapshot snapshot-name --endpoint tcp://127.0.0.1:10000 --source-volume \"/subscriptions/b9d2281e-dcd5-4dfd-9a97-xxx/resourceGroups/xxx/providers/Microsoft.Compute/disks/pvc-disk-dynamic-398b838f-0432-11e9-9978-000d3a00df41\" 13. Delete snapshot $ csc controller delete-snapshot snapshot-name --endpoint tcp://127.0.0.1:10000 14. List snapshot $ csc controller list-snapshots --endpoint tcp://127.0.0.1:10000 How to test CSI driver in a Kubernetes cluster  Build driver image and push image to dockerhub  # run `docker login` first export REGISTRY=\u003cdockerhub-alias\u003e make azuredisk-container make push-latest  Replace mcr.microsoft.com/k8s/csi/azuredisk-csi:latest in csi-azuredisk-controller.yaml and csi-azuredisk-node.yaml with above dockerhub image urls and then follow install CSI driver master version  wget -O csi-azuredisk-controller.yaml https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/csi-azuredisk-controller.yaml # edit csi-azuredisk-controller.yaml kubectl apply -f csi-azuredisk-controller.yaml wget -O csi-azuredisk-node.yaml https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/csi-azuredisk-node.yaml # edit csi-azuredisk-node.yaml kubectl apply -f csi-azuredisk-node.yaml How to update Azure cloud provider library  get latest version of github.com/kubernetes/legacy-cloud-providers   in following example, 20200619215319-3e8d72e51d7d is the git version\n # git clone https://github.com/kubernetes/legacy-cloud-providers.git # cd ~/go/src/github.com/kubernetes/legacy-cloud-providers # TZ=UTC # git --no-pager show \\ --quiet \\ --abbrev=12 \\ --date='format-local:%Y%m%d%H%M%S' \\ --format=\"%cd-%h\" 20200619215319-3e8d72e51d7d  update go.mod  export GO111MODULE=on #edit go.mod. add the necessary vendors in `replace` go mod tidy go mod vendor How to update chart index helm repo index charts --url=https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/charts ","excerpt":"Azure disk CSI driver development guide How to build this project ‚Ä¶","ref":"/azuredisk-csi-driver/development/development-guide/","title":"development-guide"},{"body":"Topology(Availability Zone) Topology is a beta feature since Kubernetes v1.14, refer to CSI Topology Feature for more details.\nCheck node topology after driver installation In below example, there are two nodes with topology label: topology.disk.csi.azure.com/zone=eastus2-1\n$ kubectl get no --show-labels | grep topo k8s-agentpool-83483713-vmss000000 Ready agent 62d v1.16.2 ...topology.disk.csi.azure.com/zone=eastus2-1 k8s-agentpool-83483713-vmss000001 Ready agent 62d v1.16.2 ...topology.disk.csi.azure.com/zone=eastus2-1 Use following storage class with topology support apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:managed-csiprovisioner:disk.csi.azure.comparameters:skuname:StandardSSD_LRS# available values: Standard_LRS, Premium_LRS, StandardSSD_LRS, UltraSSD_LRSreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumer# make sure `volumeBindingMode` is set as `WaitForFirstConsumer`Follow azure disk dynamic provisioning Continue from step Create an azuredisk CSI PVC, refer to Basic usage\nLinks  Azure Availability Zones Allowed Topologies  ","excerpt":"Topology(Availability Zone) Topology is a beta feature since ‚Ä¶","ref":"/azuredisk-csi-driver/features/topology/","title":"Topology feature"},{"body":"Azure disk could not be resized when it's in ‚Äúattached‚Äù state, we could only resize azure disk when it's in ‚Äúunattached‚Äù state, this page will show you how to achieve this. How to use Azure Disk volume expansion feature\n available from v1.11.0, details: Add azuredisk PV size grow feature\n  In the beginning, pls make sure the azure disk PVC is created by kubernetes.io/azure-disk storage class with allowVolumeExpansion: true (default is false)  kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: hdd provisioner: kubernetes.io/azure-disk parameters: skuname: Standard_LRS kind: Managed cachingmode: None allowVolumeExpansion: true  Before run kubectl edit pvc pvc-azuredisk operation, pls make sure this PVC is not mounted by any pod, otherwise there would be resize error. There are a few ways to achieve this, wait a few minutes for the PVC disk detached from the node after below operation:  option#1: change the replica count to 0, this will terminate the pod and detach the disk option#2: cordon all nodes and then delete the original pod, this will make the pod in pending state    Make sure the only pod is terminated from the agent node, otherwise may hit VolumeResizeFailed when edit disk PVC\nNow run kubectl edit pvc pvc-azuredisk to change azuredisk PVC size from 6GB to 10GB\n# Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: ... ... name: pvc-azuredisk ... spec: accessModes: - ReadWriteOnce resources: requests: storage: 6Gi storageClassName: hdd volumeMode: Filesystem volumeName: pvc-d2d00dd9-6185-11e8-a6c3-000d3a0643a8 status: accessModes: - ReadWriteOnce capacity: storage: 6Gi conditions: - lastProbeTime: null lastTransitionTime: 2018-05-27T08:14:34Z message: Waiting for user to (re-)start a pod to finish file system resize of volume on node. status: \"True\" type: FileSystemResizePending phase: Bound  After resized, run kubectl describe pvc pvc-azuredisk to check PVC status:  $ kubectl describe pvc pvc-azuredisk Name: pvc-azuredisk Namespace: default StorageClass: hdd Status: Bound ... Capacity: 5Gi Access Modes: RWO Conditions: Type Status LastProbeTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- FileSystemResizePending True Mon, 01 Jan 0001 00:00:00 +0000 Wed, 29 Aug 2018 02:29:52 +0000 Waiting for user to (re-)start a pod to finish file system resize of volume on node. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ProvisioningSucceeded 3m57s persistentvolume-controller Successfully provisioned volume pvc-d7d250c1-ab32-11e8-bfaf-000d3a4e76db using kubernetes.io/azure-disk Mounted By: \u003cnone\u003e  Create a pod mounting with this PVC, you will get  $ kubectl exec -it nginx-azuredisk -- bash # df -h Filesystem Size Used Avail Use% Mounted on ... /dev/sdf 9.8G 16M 9.3G 1% /mnt/disk ... Note: volume expansion feature is beta since v1.11\n","excerpt":"Azure disk could not be resized when it's in ‚Äúattached‚Äù state, we ‚Ä¶","ref":"/azuredisk-csi-driver/features/volume-expansion/","title":"Volume expansion feature"},{"body":"To be completed.\n","excerpt":"To be completed.\n","ref":"/azuredisk-csi-driver/development/future/","title":"Future Plans"},{"body":"Volume Cloning Example Create a Source PVC kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/storageclass-azuredisk-csi.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/pvc-azuredisk-csi.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/nginx-pod-azuredisk.yaml Check the Source PVC $ kubectl exec nginx-azuredisk -- ls /mnt/azuredisk lost+found outfile Create a PVC from an existing PVC kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/cloning/pvc-azuredisk-cloning.yaml Check the Creation Status $ kubectl describe pvc pvc-azuredisk-cloning Name: pvc-azuredisk-cloning Namespace: default StorageClass: disk.csi.azure.com Status: Bound Volume: pvc-276b72d5-adc5-45cd-ad67-2a1f8fd6c81b Labels: \u003cnone\u003e Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"annotations\":{},\"name\":\"pvc-azuredisk-cloning\",\"namespace\":\"default\"},\"spec... pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: disk.csi.azure.com Finalizers: [kubernetes.io/pvc-protection] Capacity: 10Gi Access Modes: RWO VolumeMode: Filesystem Mounted By: \u003cnone\u003e Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Provisioning 30s disk.csi.azure.com_csi-azuredisk-controller-67f97cbc57-52xpb_dc6c68b9-c45a-4fac-8497-3564fed3a59a External provisioner is provisioning volume for claim \"default/pvc-azuredisk-cloning\" Normal ExternalProvisioning 25s (x2 over 30s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \"disk.csi.azure.com\" or manually created by system administrator Normal ProvisioningSucceeded 20s disk.csi.azure.com_csi-azuredisk-controller-67f97cbc57-52xpb_dc6c68b9-c45a-4fac-8497-3564fed3a59a Successfully provisioned volume pvc-276b72d5-adc5-45cd-ad67-2a1f8fd6c81b Restore the PVC into a Pod kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/example/cloning/nginx-pod-restored-cloning.yaml Check Sample Data $ kubectl exec nginx-restored-cloning -- ls /mnt/azuredisk lost+found outfile ","excerpt":"Volume Cloning Example Create a Source PVC kubectl apply -f ‚Ä¶","ref":"/azuredisk-csi-driver/features/volume-cloning/","title":"Volume cloning feature"},{"body":"The v0.8.0 version of AzureDisk CSI Driver.\nPlease see azuredisk for guidance.\nFeatures:\n support force detach support incremental snapshot support tags in snapshot  Bug Fixes:\n fix disk API 10s latency issue fix initial delay(1s) when mount azure disk fix incorrect max azure disk max count  ","excerpt":"The v0.8.0 version of AzureDisk CSI Driver.\nPlease see azuredisk for ‚Ä¶","ref":"/azuredisk-csi-driver/blog/2020/08/10/v0.8.0/","title":"v0.8.0"},{"body":"The v0.7.0 version of AzureDisk CSI Driver.\nPlease see azuredisk for guidance.\nFeatures:\n windows support(alpha) shared disk support read cloud config from secret  ","excerpt":"The v0.7.0 version of AzureDisk CSI Driver.\nPlease see azuredisk for ‚Ä¶","ref":"/azuredisk-csi-driver/blog/2020/04/10/v0.7.0/","title":"v0.7.0"},{"body":"The v0.6.0 version of AzureDisk CSI Driver.\nPlease see azuredisk for guidance.\nFeatures:\n support migration from in-tree driver to CSI driver snapshot beta support MSI support azure disk WriteAccelerator support  ","excerpt":"The v0.6.0 version of AzureDisk CSI Driver.\nPlease see azuredisk for ‚Ä¶","ref":"/azuredisk-csi-driver/blog/2020/02/18/v0.6.0/","title":"v0.6.0"},{"body":"The beta version of AzureDisk CSI Driver.\nPlease see azuredisk for guidance.\nFeatures:\n BYOK(SSE+CMK) support topology(availability zone) support volume expansion support volume cloning support enable leader election for controller component(2 replicas by default)  ","excerpt":"The beta version of AzureDisk CSI Driver.\nPlease see azuredisk for ‚Ä¶","ref":"/azuredisk-csi-driver/blog/2020/01/06/v0.5.0/","title":"v0.5.0"},{"body":"The beta version of AzureDisk CSI Driver.\nPlease see azuredisk for guidance.\nFeatures:\n snapshot support disk partitoin support fix IMDS issue  ","excerpt":"The beta version of AzureDisk CSI Driver.\nPlease see azuredisk for ‚Ä¶","ref":"/azuredisk-csi-driver/blog/2019/10/12/v0.4.0/","title":"v0.4.0"},{"body":"The beta version of AzureDisk CSI Driver.\nPlease see azuredisk for guidance.\nFeatures:\n switch to CSI 1.1.0 switch to use kubernetes-1.15.0 azure cloud provider library fix all sanity test failures add e2e tests add snapshot support add Prometheus metrics support  ","excerpt":"The beta version of AzureDisk CSI Driver.\nPlease see azuredisk for ‚Ä¶","ref":"/azuredisk-csi-driver/blog/2019/08/06/v0.3.0/","title":"v0.3.0"},{"body":" azure disk plugin known issues  Recommended stable version for azure disk 1. disk attach error 2. disk unavailable after attach/detach a data disk on a node 3. Azure disk support on Sovereign Cloud 4. Time cost for Azure Disk PVC mount 5. Azure disk PVC Multi-Attach error, makes disk mount very slow or mount failure forever 6. WaitForAttach failed for azure disk: parsing ‚Äú/dev/disk/azure/scsi1/lun1‚Äù: invalid syntax 7. uid and gid setting in azure disk 8. Addition of a blob based disk to VM with managed disks is not supported 9. dynamic azure disk PVC try to access wrong storage account (of other resource group) 10. data loss if using existing azure disk with partitions in disk mount 11. Delete azure disk PVC which is already in use by a pod 12. create azure disk PVC failed due to account creation failure 13. cannot find Lun for disk 14. azure disk attach/detach failure, mount issue, i/o error    Recommended stable version for azure disk    k8s version stable version     v1.7 1.7.14 or later   v1.8 1.8.13 or later   v1.9 1.9.7 or later (1.9.6 on AKS)   v1.10 1.10.12 or later   v1.11 1.11.6 or later   v1.12 1.12.4 or later   v1.13 1.13.0    1. disk attach error Issue details:\nIn some corner case(detaching multiple disks on a node simultaneously), when scheduling a pod with azure disk mount from one node to another, there could be lots of disk attach error(no recovery) due to the disk not being released in time from the previous node. This issue is due to lack of lock before DetachDisk operation, actually there should be a central lock for both AttachDisk and DetachDisk operations, only one AttachDisk or DetachDisk operation is allowed at one time.\nThe disk attach error could be like following:\nCannot attach data disk 'cdb-dynamic-pvc-92972088-11b9-11e8-888f-000d3a018174' to VM 'kn-edge-0' because the disk is currently being detached or the last detach operation failed. Please wait until the disk is completely detached and then try again or delete/detach the disk explicitly again. Related issues\n Azure Disk Detach are not working with multiple disk detach on the same Node Azure disk fails to attach and mount, causing rescheduled pod to stall following node disruption Since Intel CPU Azure update, new Azure Disks are not mounting, very critical‚Ä¶  Busy azure-disk regularly fail to mount causing K8S Pod deployments to halt  Mitigation:\n option#1: Update every agent node that has attached or detached the disk in problem  In Azure cloud shell, run\n$vm = Get-AzureRMVM -ResourceGroupName $rg -Name $vmname Update-AzureRmVM -ResourceGroupName $rg -VM $vm -verbose -debug In Azure cli, run\naz vm update -g \u003cgroup\u003e -n \u003cname\u003e  option#2:   kubectl cordon node #make sure no scheduling on this node kubectl drain node #schedule pod in current node to other node restart the Azure VM for node via the API or portal, wait until VM is ‚ÄúRunning‚Äù kubectl uncordon node  Fix\n PR fix race condition issue when detaching azure disk has fixed this issue by add a lock before DetachDisk     k8s version fixed version     v1.6 no fix since v1.6 does not accept any cherry-pick   v1.7 1.7.14   v1.8 1.8.9   v1.9 1.9.5   v1.10 1.10.0    2. disk unavailable after attach/detach a data disk on a node  üí° NOTE: Azure platform has fixed the host cache issue, the suggested host cache setting of data disk is ReadOnly now, more details about azure disk cache setting Issue details:\n From k8s v1.7, default host cache setting changed from None to ReadWrite, this change would lead to device name change after attach multiple disks on a node, finally lead to disk unavailable from pod. When access data disk inside a pod, will get following error:\n[root@admin-0 /]# ls /datadisk ls: reading directory .: Input/output error In my testing on Ubuntu 16.04 D2_V2 VM, when attaching the 6th data disk will cause device name change on agent node, e.g. following lun0 disk should be sdc other than sdk.\nazureuser@k8s-agentpool2-40588258-0:~$ tree /dev/disk/azure ... √¢‚Äù‚Äù√¢‚Äù‚Ç¨√¢‚Äù‚Ç¨ scsi1 √¢‚Äù≈ì√¢‚Äù‚Ç¨√¢‚Äù‚Ç¨ lun0 -\u003e ../../../sdk √¢‚Äù≈ì√¢‚Äù‚Ç¨√¢‚Äù‚Ç¨ lun1 -\u003e ../../../sdj √¢‚Äù≈ì√¢‚Äù‚Ç¨√¢‚Äù‚Ç¨ lun2 -\u003e ../../../sde √¢‚Äù≈ì√¢‚Äù‚Ç¨√¢‚Äù‚Ç¨ lun3 -\u003e ../../../sdf √¢‚Äù≈ì√¢‚Äù‚Ç¨√¢‚Äù‚Ç¨ lun4 -\u003e ../../../sdg √¢‚Äù≈ì√¢‚Äù‚Ç¨√¢‚Äù‚Ç¨ lun5 -\u003e ../../../sdh √¢‚Äù‚Äù√¢‚Äù‚Ç¨√¢‚Äù‚Ç¨ lun6 -\u003e ../../../sdi Related issues\n device name change due to azure disk host cache setting unable to use azure disk in StatefulSet since /dev/sd* changed after detach/attach disk Disk error when pods are mounting a certain amount of volumes on a node unable to use azure disk in StatefulSet since /dev/sd* changed after detach/attach disk Input/output error when accessing PV PersistentVolumeClaims changing to Read-only file system suddenly  Workaround:\n add cachingmode: None in azure disk storage class(default is ReadWrite), e.g.  kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:hddprovisioner:kubernetes.io/azure-diskparameters:skuname:Standard_LRSkind:Managedcachingmode:NoneFix\n PR fix device name change issue for azure disk could fix this issue too, it will change default cachingmode value from ReadWrite to None.     k8s version fixed version     v1.6 no such issue as cachingmode is already None by default   v1.7 1.7.14   v1.8 1.8.11   v1.9 1.9.4   v1.10 1.10.0    3. Azure disk support on Sovereign Cloud Fix\n PR Azure disk on Sovereign Cloud fixed this issue     k8s version fixed version     v1.7 1.7.9   v1.8 1.8.3   v1.9 1.9.0   v1.10 1.10.0    4. Time cost for Azure Disk PVC mount Original time cost for Azure Disk PVC mount on a standard node size(e.g. Standard_D2_V2) is around 1 minute, podAttachAndMountTimeout is 2 minutes, total waitForAttachTimeout is 10 minutes, so a disk remount(detach and attach in sequential) would possibly cost more than 2min, thus may fail.\n Note: for some smaller VM size which has only 1 CPU core, time cost would be much bigger(e.g. \u003e 10min) since container is hard to get CPU slot.\n Related issues\n ‚Äòtimeout expired waiting for volumes to attach/mount for pod when cluster‚Äô when node-vm-size is Standard_B1s  Fix\n PR using cache fix fixed this issue, which could reduce the mount time cost to around 30s.     k8s version fixed version     v1.8 no fix   v1.9 1.9.2   v1.10 1.10.0    5. Azure disk PVC Multi-Attach error, makes disk mount very slow or mount failure forever Issue details:\nWhen schedule a pod with azure disk volume from one node to another, total time cost of detach \u0026 attach is around 1 min from v1.9.2, while in v1.9.x, there is an UnmountDevice failure issue in containerized kubelet which makes disk mount very slow or mount failure forever, this issue only exists in v1.9.x due to PR Refactor nsenter, v1.10.0 won't have this issue since devicePath is updated in v1.10 code\nerror logs:\n kubectl describe po POD-NAME  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m default-scheduler Successfully assigned deployment-azuredisk1-6cd8bc7945-kbkvz to k8s-agentpool-88970029-0 Warning FailedAttachVolume 3m attachdetach-controller Multi-Attach error for volume \"pvc-6f2d0788-3b0b-11e8-a378-000d3afe2762\" Volume is already exclusively attached to one node and can't be attached to another Normal SuccessfulMountVolume 3m kubelet, k8s-agentpool-88970029-0 MountVolume.SetUp succeeded for volume \"default-token-qt7h6\" Warning FailedMount 1m kubelet, k8s-agentpool-88970029-0 Unable to mount volumes for pod \"deployment-azuredisk1-6cd8bc7945-kbkvz_default(5346c040-3e4c-11e8-a378-000d3afe2762)\": timeout expired waiting for volumes to attach/mount for pod \"default\"/\"deployment-azuredisk1-6cd8bc7945-kbkvz\". list of unattached/unmounted volumes=[azuredisk]  kubelet logs from the new node  E0412 20:08:10.920284 7602 nestedpendingoperations.go:263] Operation for \"\\\"kubernetes.io/azure-disk//subscriptions/xxx/resourceGroups/MC_xxx_eastus/providers/Microsoft.Compute/disks/kubernetes-dynamic-pvc-11035a31-3e8d-11e8-82ec-0a58ac1f04cf\\\"\" failed. No retries permitted until 2018-04-12 20:08:12.920234762 +0000 UTC m=+1467.278612421 (durationBeforeRetry 2s). Error: \"Volume has not been added to the list of VolumesInUse in the node's volume status for volume \\\"pvc-11035a31-3e8d-11e8-82ec-0a58ac1f04cf\\\" (UniqueName: \\\"kubernetes.io/azure-disk//subscriptions/xxx/resourceGroups/MC_xxx_eastus/providers/Microsoft.Compute/disks/kubernetes-dynamic-pvc-11035a31-3e8d-11e8-82ec-0a58ac1f04cf\\\") pod \\\"symbiont-node-consul-0\\\" (UID: \\\"11043b12-3e8d-11e8-82ec-0a58ac1f04cf\\\") \" Related issues\n UnmountDevice would fail in containerized kubelet upgrade k8s process is broke  Mitigation:\nIf azure disk PVC mount successfully in the end, there is no action, while if it could not be mounted for more than 20min, following actions could be taken:\n check whether volumesInUse list has unmounted azure disks, run:  kubectl get no NODE-NAME -o yaml \u003e node.log all volumes in volumesInUse should be also in volumesAttached, otherwise there would be issue\n restart kubelet on the original node would solve this issue: sudo kubectl kubelet restart  Fix\n PR fix nsenter GetFileType issue in containerized kubelet fixed this issue     k8s version fixed version     v1.8 no such issue   v1.9 v1.9.7   v1.10 no such issue    After fix in v1.9.7, it took about 1 minute for scheduling one azure disk mount from one node to another, you could find details here.\nSince azure disk attach/detach operation on a VM cannot be parallel, scheduling 3 azure disk mounts from one node to another would cost about 3 minutes.\n6. WaitForAttach failed for azure disk: parsing ‚Äú/dev/disk/azure/scsi1/lun1‚Äù: invalid syntax Issue details: MountVolume.WaitForAttach may fail in the azure disk remount\nerror logs:\nin v1.10.0 \u0026 v1.10.1, MountVolume.WaitForAttach will fail in the azure disk remount, error logs would be like following:\n incorrect DevicePath format on Linux  MountVolume.WaitForAttach failed for volume \"pvc-f1562ecb-3e5f-11e8-ab6b-000d3af9f967\" : azureDisk - Wait for attach expect device path as a lun number, instead got: /dev/disk/azure/scsi1/lun1 (strconv.Atoi: parsing \"/dev/disk/azure/scsi1/lun1\": invalid syntax) Warning FailedMount 1m (x10 over 21m) kubelet, k8s-agentpool-66825246-0 Unable to mount volumes for pod  wrong DevicePath(LUN) number on Windows   Warning FailedMount 1m kubelet, 15282k8s9010 MountVolume.WaitForAttach failed for volume \"disk01\" : azureDisk - WaitForAttach failed within timeout node (15282k8s9010) diskId:(andy-mghyb 1102-dynamic-pvc-6c526c51-4a18-11e8-ab5c-000d3af7b38e) lun:(4) Related issues\n WaitForAttach failed for azure disk: parsing ‚Äú/dev/disk/azure/scsi1/lun1‚Äù: invalid syntax Pod unable to attach PV after being deleted (Wait for attach expect device path as a lun number, instead got: /dev/disk/azure/scsi1/lun0 (strconv.Atoi: parsing ‚Äú/dev/disk/azure/scsi1/lun0‚Äù: invalid syntax)  Fix\n PR fix WaitForAttach failure issue for azure disk fixed this issue     k8s version fixed version     v1.8 no such issue   v1.9 no such issue   v1.10 1.10.2    7. uid and gid setting in azure disk Issue details: Unlike azure file mountOptions, you will get following failure if set mountOptions like uid=999,gid=999 in azure disk mount:\nazureDisk - mountDevice:FormatAndMount failed with exit status 32 That's because azureDisk use ext4 file system by default, mountOptions like [uid=x,gid=x] could not be set in mount time.\nRelated issues\n Timeout expired waiting for volumes to attach  Solution:\n option#1: Set uid in runAsUser and gid in fsGroup for pod: security context for a Pod  e.g. Following setting will set pod run as root, make it accessible to any file:\napiVersion:v1kind:Podmetadata:name:security-context-demospec:securityContext:runAsUser:0fsGroup:0 Note: Since gid \u0026 uid is mounted as 0(root) by default, if set as non-root(e.g. 1000), k8s will use chown to change all dir/files under that disk, this is a time consuming job, which would make mount device very slow, in this issue: Timeout expired waiting for volumes to attach, it costs about 10 min for chown operation complete.\n  option#2: use chown in initContainers  initContainers: - name: volume-mount image: busybox command: [\"sh\", \"-c\", \"chown -R 100:100 /data\"] volumeMounts: - name: \u003cyour data volume\u003e mountPath: /data 8. Addition of a blob based disk to VM with managed disks is not supported Issue details:\nFollowing error may occur if attach a blob based(unmanaged) disk to VM with managed disks:\nWarning FailedMount 42s (x2 over 1m) attachdetach AttachVolume.Attach failed for volume \"pvc-f17e5e77-474e-11e8-a2ea-000d3a10df6d\" : Attach volume \"holo-k8s-dev-dynamic-pvc-f17e5e77-474e-11e8-a2ea-000d3a10df6d\" to instance \"k8s-master-92699158-0\" failed with compute.VirtualMachinesClient#CreateOrUpdate: Failure responding to request: StatusCode=409 -- Original Error: autorest/azure: Service returned an error. Status=409 Code=\"OperationNotAllowed\" Message=\"Addition of a blob based disk to VM with managed disks is not supported.\" This issue is by design as in Azure, there are two kinds of disks, blob based(unmanaged) disk and managed disk, an Azure VM could not attach both of these two kinds of disks.\nSolution:\nUse default azure disk storage class in aks-engine, as default will always be identical to the agent pool, that is, if VM is managed, it will be managed azure disk class, if unmanaged, then it's unmanaged disk class.\n9. dynamic azure disk PVC try to access wrong storage account (of other resource group) Issue details:\nIn a k8s cluster with blob based VMs(won't happen in AKS since AKS only use managed disk), create dynamic azure disk PVC may fail, error logs is like following:\nFailed to provision volume with StorageClass \"default\": azureDisk - account ds6c822a4d484211eXXXXXX does not exist while trying to create/ensure default container Related issues\n Multiple clusters - dynamic PVCs try to access wrong storage account (of other resource group)  Fix\n PR fix storage account not found issue: use ListByResourceGroup instead of List() fixed this issue     k8s version fixed version     v1.8 1.8.13   v1.9 1.9.9   v1.10 no such issue    Work around:\nthis bug only exists in blob based VM in v1.8.x, v1.9.x, so if specify ManagedDisks when creating k8s cluster in aks-engine(AKS is using managed disk by default), it won't have this issue:\n\"agentPoolProfiles\": [ { ... \"storageProfile\" : \"ManagedDisks\", ... } 10. data loss if using existing azure disk with partitions in disk mount Issue details:\nWhen use an existing azure disk(also called static provisioning) in pod, if that disk has partitions, the disk will be formatted in the pod mounting process, actually k8s volume don't support mount disk with partitions, disk mount would fail finally. While for mounting existing azure disk that has partitions, data will be lost since it will format that disk first. This issue happens only on Linux.\nRelated issues\n data loss if using existing azure disk with partitions in disk mount  Fix\n PR fix data loss issue if using existing azure disk with partitions in disk mount will let azure provider return error when mounting existing azure disk that has partitions     k8s version fixed version     v1.8 1.8.15   v1.9 1.9.11   v1.10 1.10.5   v1.11 1.11.0    Work around:\nDon't use existing azure disk that has partitions, e.g. following disk in LUN 0 that has one partition:\nazureuser@aks-nodepool1-28371372-0:/$ ls -l /dev/disk/azure/scsi1/ total 0 lrwxrwxrwx 1 root root 12 Apr 27 08:04 lun0 -\u003e ../../../sdc lrwxrwxrwx 1 root root 13 Apr 27 08:04 lun0-part1 -\u003e ../../../sdc1 11. Delete azure disk PVC which is already in use by a pod Issue details:\nFollowing error may occur if delete azure disk PVC which is already in use by a pod:\nkubectl describe pv pvc-d8eebc1d-74d3-11e8-902b-e22b71bb1c06 ... Message: disk.DisksClient#Delete: Failure responding to request: StatusCode=409 -- Original Error: autorest/azure: Service returned an error. Status=409 Code=\"OperationNotAllowed\" Message=\"Disk kubernetes-dynamic-pvc-d8eebc1d-74d3-11e8-902b-e22b71bb1c06 is attached to VM /subscriptions/{subs-id}/resourceGroups/MC_markito-aks-pvc_markito-aks-pvc_westus/providers/Microsoft.Compute/virtualMachines/aks-agentpool-25259074-0.\" Fix:\nThis is a common k8s issue, other cloud provider would also has this issue. There is a PVC protection feature to prevent this, it's alpha in v1.9, and beta(enabled by default) in v1.10\nWork around: delete pod first and then delete azure disk pvc after a few minutes\n12. create azure disk PVC failed due to account creation failure  please note this issue only happens on unmanaged k8s cluster\n Issue details: User may get Account property kind is invalid for the request error when trying to create a new unmanaged azure disk PVC, error would be like following:\nazureuser@k8s-master-17140924-0:/tmp$ kubectl describe pvc Name: pvc-azuredisk Namespace: default StorageClass: hdd Status: Bound ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ProvisioningFailed 31m persistentvolume-controller Failed to provision volume with StorageClass \"hdd\": Create Storage Account: ds10e15ed89c5811e8a0a70, error: storage.AccountsClient#Create: Failure sending request: StatusCode=400 -- Original Error: Code=\"AccountPropertyIsInvalid\" Message=\"Account property kind is invalid for the request.\" Fix\n PR fix azure disk create failure due to sdk upgrade fixed this issue     k8s version fixed version     v1.9 no such issue   v1.10 no such issue   v1.11 1.11.3   v1.12 no such issue    Work around:\n create a storage account and specify that account in azure disk storage class, e.g.  kind:StorageClassapiVersion:storage.k8s.io/v1beta1metadata:name:ssdprovisioner:kubernetes.io/azure-diskparameters:skuname:Premium_LRSstorageAccount:customerstorageaccountkind:Dedicated13. cannot find Lun for disk Issue details:\nFollowing error may occur if attach a disk to a node:\nMountVolume.WaitForAttach failed for volume \"pvc-12b458f4-c23f-11e8-8d27-46799c22b7c6\" : Cannot find Lun for disk kubernetes-dynamic-pvc-12b458f4-c23f-11e8-8d27-46799c22b7c6 Related issues\n GetAzureDiskLun sometimes costs 1 min which is too long time  Fix\n PR fix azure disk attachment error on Linux will extract the LUN num from device path only on Linux     k8s version fixed version     v1.9 no such issue   v1.10 1.10.10   v1.11 1.11.5   v1.12 1.12.3   v1.13 no such issue    Work around:\nwait for a few more minutes should work\n14. azure disk attach/detach failure, mount issue, i/o error Issue details:\nWe found a disk attach/detach issue due to dirty vm cache PR introduced from v1.9.2, it would lead to following disk issues:\n disk attach/detach failure for a long time disk I/O error unexpected disk detachment from VM VM running into failed state due to attaching non-existing disk   Note: above error may only happen when there are multiple disk attach/detach operations in parallel and it's not easy to repro since it happens on a little possibility.\n Related issues\n Azure Disks volume attach still times out on Kubernetes 1.10 Azure Disks occasionally mounted in a way leading to I/O errors  Fix\nWe changed the azure disk attach/detach retry logic in k8s v1.13, switch to use k8s attach-detach controller to do attach/detach disk retry and clean vm cache after every disk operation, this issue is proved to be fixed in our disk attach/detach stress test and also verified in customer env:\n PR remove retry operation on attach/detach azure disk in azure cloud provider PR fix azure disk attach/detach failed forever issue PR fix detach azure disk issue due to dirty cache     k8s version fixed version     v1.9 issue introduced in v1.9.2, no cherry-pick fix allowed   v1.10 1.10.12   v1.11 1.11.6   v1.12 1.12.4   v1.13 no such issue    Work around:\n if there is attach disk failure for long time, restart controller manager may work if there is disk not detached for long time, detach that disk manually  ","excerpt":" azure disk plugin known issues  Recommended stable version for azure ‚Ä¶","ref":"/azuredisk-csi-driver/faq/known-issues/azuredisk/","title":"AzureDisk CSI Driver Known Issues"},{"body":" AzureDisk CSI Driver This driver allows Kubernetes to use Azure disk volume, csi plugin name: disk.csi.azure.com\nGet Started   Contribute   \n          Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more ‚Ä¶\n   Join our slack channel! Please join #provider-azure in Kubernetes slack workspace.\nRead more ‚Ä¶\n   Check out our release notes! For announcement of latest features, etc.\nRead more ‚Ä¶\n    ","excerpt":" AzureDisk CSI Driver This driver allows Kubernetes to use Azure disk ‚Ä¶","ref":"/azuredisk-csi-driver/","title":"AzureDisk CSI driver"},{"body":"End to End Test Run E2E tests Locally Prerequisite  Make sure a kubernetes cluster(with version \u003e= 1.13) is set up and kubeconfig is under $HOME/.kube/config Copy /etc/kubernetes/azure.json from agent node to local dev machine where you are going to run e2e tests  How to run E2E tests # testing against CSI Driver by default make e2e-test # Run Windows e2e tests export TEST_WINDOWS=\"true\" make e2e-test # Run specific e2e tests go test -v -timeout=0 ./test/e2e -ginkgo.noColor -ginkgo.v -ginkgo.focus=\"deployment\"  testing against in-tree volume driver\n export AZURE_STORAGE_DRIVER=\"kubernetes.io/azure-disk\" make e2e-test ","excerpt":"End to End Test Run E2E tests Locally Prerequisite  Make sure a ‚Ä¶","ref":"/azuredisk-csi-driver/development/e2e/e2e-tests-azuredisk/","title":"AzureDisk E2E tests"},{"body":"","excerpt":"","ref":"/azuredisk-csi-driver/contribute/","title":"Contribution"},{"body":"","excerpt":"","ref":"/azuredisk-csi-driver/development/","title":"Development Guide"},{"body":"","excerpt":"","ref":"/azuredisk-csi-driver/example/","title":"Example"},{"body":"What is CSI Driver? The Container Storage Interface (CSI) is a standard for exposing arbitrary block and file storage systems to containerized workloads on Container Orchestration Systems (COs) like Kubernetes.\nUsing CSI third-party storage providers can write and deploy plugins exposing new storage systems in Kubernetes without ever having to touch the core Kubernetes code.\nGoals?  Define Kubernetes API for interacting with an arbitrary, third-party CSI volume drivers. Define mechanism by which Kubernetes master and node components will securely communicate with an arbitrary, third-party CSI volume drivers. Define mechanism by which Kubernetes master and node components will discover and register an arbitrary, third-party CSI volume driver deployed on Kubernetes. Recommend packaging requirements for Kubernetes compatible, third-party CSI Volume drivers. Recommend deployment process for Kubernetes compatible, third-party CSI Volume drivers on a Kubernetes cluster.  Non-Goals?  Replace Flex Volume plugin The Flex volume plugin exists as an exec based mechanism to create ‚Äúout-of-tree‚Äù volume plugins. Because Flex drivers exist and depend on the Flex interface, it will continue to be supported with a stable API. The CSI Volume plugin will co-exist with Flex volume plugin.  ","excerpt":"What is CSI Driver? The Container Storage Interface (CSI) is a ‚Ä¶","ref":"/azuredisk-csi-driver/faq/","title":"FAQ"},{"body":"","excerpt":"","ref":"/azuredisk-csi-driver/features/","title":"Features"},{"body":"","excerpt":"","ref":"/azuredisk-csi-driver/install/","title":"Deploy AzureDisk CSI Driver"},{"body":"Prerequisite   An azure service principal\nPlease follow this guide for creating an azure service principal The service principal should either have:\n Contributor permission of a subscription Contributor permission of a resource group. In this case, please create the resource group first    Docker daemon enabled\n  How to run Kubernetes e2e tests locally  Prepare dependency project    aks-engine\nBinary downloads for the latest version of aks-engine for are available on Github. Download AKS Engine for your operating system, extract the binary and copy it to your $PATH.\nOn macOS, you can install aks-engine with Homebrew. Run the command brew install Azure/aks-engine/aks-engine to do so. You can install Homebrew following the instructions.\nOn Windows, you can install aks-engine via Chocolatey by executing the command choco install aks-engine. You can install Chocolatey following the instructions.\nOn Linux, it could also be installed by following commands:\n$ curl -o get-akse.sh https://raw.githubusercontent.com/Azure/aks-engine/master/scripts/get-akse.sh $ chmod 700 get-akse.sh $ ./get-akse.sh   Kubernetes\nThis serves as E2E tests case source, it should be located at $GOPATH/src/k8s.io/kubernetes.\ncd $GOPATH/src go get -d k8s.io/kubernetes   kubectl\nKubectl allows you to run command against Kubernetes cluster, which is also used for deploying CSI plugins. You can follow here to install kubectl. e.g. on Linux\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin/    Build docker image azure-cloud-controller-manager and push it to your docker image repository.\ngit clone https://github.com/kubernetes-sigs/cloud-provider-azure $GOPATH/src/sigs.k8s.io/cloud-provider-azure cd $GOPATH/src/sigs.k8s.io/cloud-provider-azure export IMAGE_REGISTRY=\u003cusername\u003e export IMAGE_TAG=\u003ctag\u003e make build-images make push-images # or manually `docker push`   Deploy a Kubernetes cluster with the above azure-cloud-controller-manager image.\nTo deploy a cluster, export all the required environmental variables first and then invoke make deploy:\nexport RESOURCE_GROUP_NAME=\u003cresource group name\u003e export K8S_AZURE_LOCATION=\u003clocation\u003e export K8S_AZURE_SUBSID=\u003csubscription ID\u003e export K8S_AZURE_SPID=\u003cclient id\u003e export K8S_AZURE_SPSEC=\u003cclient secret\u003e export K8S_AZURE_TENANTID=\u003ctenant id\u003e export USE_CSI_DEFAULT_STORAGECLASS=\u003ctrue/false\u003e export K8S_RELEASE_VERSION=\u003ck8s release version\u003e export CCM_IMAGE=\u003cimage of the cloud controller manager\u003e export CNM_IMAGE=\u003cimage of the cloud node manager\u003e make deploy To connect the cluster:\nexport KUBECONFIG=$GOPATH/src/sigs.k8s.io/cloud-provider-azure/_output/$(ls -t _output | head -n 1)/kubeconfig/kubeconfig.$LOCATION.json kubectl cluster-info   To check out more of the deployed cluster , replace kubectl cluster-info with other kubectl commands. To further debug and diagnose cluster problems, use kubectl cluster-info dump\nGet kubetest binary  go get -u k8s.io/test-infra/kubetest Run E2E tests  Please first ensure the kubernetes project locates at $GOPATH/src/k8s.io/kubernetes, the e2e tests will be built from that location.\ncd $GOPATH/src/k8s.io/kubernetes make WHAT='test/e2e/e2e.test' make WHAT=cmd/kubectl make ginkgo export KUBERNETES_PROVIDER=azure export KUBERNETES_CONFORMANCE_TEST=y export KUBERNETES_CONFORMANCE_PROVIDER=azure export CLOUD_CONFIG=$GOPATH/src/sigs.k8s.io/cloud-provider-azure/tests/k8s-azure/manifest/azure.json # some test cases require ssh configurations export KUBE_SSH_KEY_PATH=path/to/ssh/privatekey export KUBE_SSH_USER={ssh_user} # Replace the test_args with your own. kubetest --test --provider=local --check-version-skew=false --test_args='--ginkgo.focus=Port\\sforwarding' ","excerpt":"Prerequisite   An azure service principal\nPlease follow this guide for ‚Ä¶","ref":"/azuredisk-csi-driver/development/e2e/e2e-tests/","title":"Kubernetes E2E tests"},{"body":"","excerpt":"","ref":"/azuredisk-csi-driver/blog/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/azuredisk-csi-driver/search/","title":"Search Results"}]